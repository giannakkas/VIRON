#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VIRON â€” Download GGUF Models for Jetson Orin Nano
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Downloads Q4_K_M quantized models for local inference via llama.cpp
#
# Models directory: /models (configurable via MODELS_DIR env var)
# Storage required: ~2GB (Gemma) + ~5GB (Mistral) â‰ˆ 7GB total
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -e

MODELS_DIR="${MODELS_DIR:-/models}"
mkdir -p "$MODELS_DIR"

echo ""
echo "ğŸ¤– VIRON Model Downloader"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  Target: $MODELS_DIR"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# â”€â”€â”€ Check for HuggingFace CLI or wget â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HAS_HF=false
HAS_WGET=false
if command -v huggingface-cli &>/dev/null; then HAS_HF=true; fi
if command -v wget &>/dev/null; then HAS_WGET=true; fi

if ! $HAS_WGET && ! $HAS_HF; then
    echo "âŒ Neither wget nor huggingface-cli found."
    echo "   Install: apt install wget  OR  pip install huggingface_hub"
    exit 1
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL 1: Gemma 2 2B IT (Router â€” intent classification)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMMA_FILE="gemma-2-2b-it-Q4_K_M.gguf"
GEMMA_URL="https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf"
GEMMA_REPO="bartowski/gemma-2-2b-it-GGUF"

if [ -f "$MODELS_DIR/$GEMMA_FILE" ]; then
    SIZE=$(du -h "$MODELS_DIR/$GEMMA_FILE" | cut -f1)
    echo "âœ… Gemma 2 2B IT already exists ($SIZE)"
else
    echo "ğŸ“¥ Downloading Gemma 2 2B IT (Q4_K_M) â€” ~1.5GB..."
    echo "   Source: $GEMMA_REPO"
    if $HAS_WGET; then
        wget -q --show-progress -O "$MODELS_DIR/$GEMMA_FILE" "$GEMMA_URL"
    elif $HAS_HF; then
        huggingface-cli download "$GEMMA_REPO" "$GEMMA_FILE" \
            --local-dir "$MODELS_DIR" --local-dir-use-symlinks False
    fi
    if [ -f "$MODELS_DIR/$GEMMA_FILE" ]; then
        SIZE=$(du -h "$MODELS_DIR/$GEMMA_FILE" | cut -f1)
        echo "âœ… Gemma 2 2B IT downloaded ($SIZE)"
    else
        echo "âŒ Gemma download failed!"
        echo ""
        echo "   Manual download:"
        echo "   wget -O $MODELS_DIR/$GEMMA_FILE \\"
        echo "     $GEMMA_URL"
    fi
fi

echo ""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL 2: Mistral 7B Instruct v0.3 (Tutor â€” offline answers)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MISTRAL_FILE="Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"
MISTRAL_URL="https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"
MISTRAL_REPO="bartowski/Mistral-7B-Instruct-v0.3-GGUF"

if [ -f "$MODELS_DIR/$MISTRAL_FILE" ]; then
    SIZE=$(du -h "$MODELS_DIR/$MISTRAL_FILE" | cut -f1)
    echo "âœ… Mistral 7B Instruct already exists ($SIZE)"
else
    echo "ğŸ“¥ Downloading Mistral 7B Instruct v0.3 (Q4_K_M) â€” ~4.4GB..."
    echo "   Source: $MISTRAL_REPO"
    if $HAS_WGET; then
        wget -q --show-progress -O "$MODELS_DIR/$MISTRAL_FILE" "$MISTRAL_URL"
    elif $HAS_HF; then
        huggingface-cli download "$MISTRAL_REPO" "$MISTRAL_FILE" \
            --local-dir "$MODELS_DIR" --local-dir-use-symlinks False
    fi
    if [ -f "$MODELS_DIR/$MISTRAL_FILE" ]; then
        SIZE=$(du -h "$MODELS_DIR/$MISTRAL_FILE" | cut -f1)
        echo "âœ… Mistral 7B Instruct downloaded ($SIZE)"
    else
        echo "âŒ Mistral download failed!"
        echo ""
        echo "   Manual download:"
        echo "   wget -O $MODELS_DIR/$MISTRAL_FILE \\"
        echo "     $MISTRAL_URL"
    fi
fi

echo ""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SUMMARY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  Model Files:"
ls -lh "$MODELS_DIR"/*.gguf 2>/dev/null || echo "  (no .gguf files found)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "  Next steps:"
echo "  1. Build llama.cpp with CUDA (see scripts/build_llamacpp.sh)"
echo "  2. docker-compose up  (or run manually)"
echo ""
