# ═══════════════════════════════════════════════════════════════
# VIRON — Hybrid AI Configuration
# ═══════════════════════════════════════════════════════════════
# Copy to .env and fill in your API keys:  cp .env.example .env

# ─── Cloud API Keys (leave empty to disable a provider) ────
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=

# ─── Cloud Models ──────────────────────────────────────────
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_MODEL=claude-sonnet-4-20250514
GEMINI_MODEL=gemini-2.0-flash

# ─── Local LLM (llama.cpp servers) ────────────────────────
# These are set automatically in docker-compose.
# Override only for bare-metal (non-docker) runs:
ROUTER_URL=http://localhost:8081
TUTOR_URL=http://localhost:8082

# ─── Timeouts (seconds) ───────────────────────────────────
ROUTER_TIMEOUT=10
TUTOR_TIMEOUT=30
CLOUD_TIMEOUT=40

# ─── Database ─────────────────────────────────────────────
DB_PATH=/data/viron.db

# ─── Gateway Server ──────────────────────────────────────
GATEWAY_PORT=8080

# ─── Models Directory (for download script) ──────────────
MODELS_DIR=/models

# ─── Existing VIRON Backend Config ────────────────────────
# These are used by the original Flask backend + ai-router:
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:3b
LOCAL_MODEL=phi3
CLOUD_STRATEGY=priority
DEFAULT_AGE_MODE=kids
PORT=8000
